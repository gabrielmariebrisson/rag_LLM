import os
import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from mistralai import Mistral
from dotenv import load_dotenv
import numpy as np
import re
import pandas as pd
import plotly.graph_objects as go
from deep_translator import GoogleTranslator

LANGUAGES = {
    "fr": "ğŸ‡«ğŸ‡· FranÃ§ais",
    "en": "ğŸ‡¬ğŸ‡§ English",
    "es": "ğŸ‡ªğŸ‡¸ EspaÃ±ol",
    "de": "ğŸ‡©ğŸ‡ª Deutsch",
    "it": "ğŸ‡®ğŸ‡¹ Italiano",
    "pt": "ğŸ‡µğŸ‡¹ PortuguÃªs",
    "ja": "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª",
    "zh-CN": "ğŸ‡¨ğŸ‡³ ä¸­æ–‡",
    "ar": "ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "ru": "ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹"
}

# Configuration de la page - DOIT Ãªtre en premier
st.set_page_config(page_title="RAG System with SQuAD", layout="wide")

# Initialisation de la langue
if 'language' not in st.session_state:
    st.session_state.language = 'fr'

# SÃ©lecteur de langue
lang = st.sidebar.selectbox(
    "ğŸŒ Language / Langue", 
    options=list(LANGUAGES.keys()),
    format_func=lambda x: LANGUAGES[x],
    index=list(LANGUAGES.keys()).index(st.session_state.language)
)

st.session_state.language = lang

# Cache pour les traductions
if 'translations_cache' not in st.session_state:
    st.session_state.translations_cache = {}

def _(text, source_lang='fr'):
    """Fonction de traduction automatique avec cache"""
    if lang == source_lang:
        return text
    
    cache_key = f"{source_lang}_{lang}_{text}"
    if cache_key in st.session_state.translations_cache:
        return st.session_state.translations_cache[cache_key]
    
    try:
        translated = GoogleTranslator(source=source_lang, target=lang).translate(text)
        st.session_state.translations_cache[cache_key] = translated
        return translated
    except:
        return text

load_dotenv()
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL_NAME = os.getenv("MISTRAL_MODEL_NAME", "mistral-tiny-2407")

@st.cache_resource
def load_vectorstore():
    """Charge le vectorstore FAISS sauvegardÃ©"""
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    return vectorstore

try:
    vectorstore = load_vectorstore()
except Exception as e:
    st.error(_(f"âŒ Erreur lors du chargement du vectorstore: {e}"))
    st.stop()

def rag_query(query, k=5):
    """Fonction RAG utilisant Mistral API"""
    if not MISTRAL_API_KEY or MISTRAL_API_KEY.strip() == "":
        raise ValueError(_("MISTRAL_API_KEY is not set or is empty. Please set it in your .env file."))
    
    results = vectorstore.similarity_search(query, k=k)
    context = "\n\n".join([doc.page_content for doc in results])
    
    messages = [
        {"role": "system", "content": "You are a helpful assistant. Write responses in complete, well-developed sentences. Express ideas clearly and naturally, avoiding overly brief or list-style answers."},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]
    
    with Mistral(api_key=MISTRAL_API_KEY) as mistral:
        response = mistral.chat.complete(
            model=MISTRAL_MODEL_NAME, 
            messages=messages, 
            stream=False
        )
    
    return {
        "response": response.choices[0].message.content,
        "retrieved_documents": results,
        "context": context
    }

# Bouton de redirection
st.markdown(
    f"""
    <a href="https://gabriel.mariebrisson.fr" target="_blank" style="text-decoration:none;">
    <div style="
    display: inline-block;
    background: linear-gradient(135deg, #6A11CB 0%, #2575FC 100%);
    color: white;
    padding: 12px 25px;
    border-radius: 30px;
    text-align: center;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    box-shadow: 0 4px 15px rgba(37, 117, 252, 0.3);
    transition: all 0.3s ease;
    text-transform: uppercase;
    letter-spacing: 1px;
    border: 2px solid transparent;
    position: relative;
    overflow: hidden;
    ">
    {_("Retour")}
    </div>
    </a>
    """,
    unsafe_allow_html=True
)

st.title(_("RAG System with SQuAD Dataset"))
st.markdown(_("""
Il s'agit d'un systÃ¨me de gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG) construit avec :
- **ModÃ¨le d'embedding** : sentence-transformers/all-MiniLM-L6-v2  
- **Base de donnÃ©es vectorielle** : FAISS (LangChain)  
- **ModÃ¨le de langage (LLM)** : API Mistral  
- **Jeu de donnÃ©es** : SQuAD 2.0
"""))

if not MISTRAL_API_KEY:
    st.error(_("âš ï¸ MISTRAL_API_KEY n'est pas dÃ©finie. Veuillez la configurer dans votre fichier .env"))
    st.stop()

if 'history' not in st.session_state:
    st.session_state.history = []

st.sidebar.header(_("âš™ï¸ ParamÃ¨tres"))
num_results = st.sidebar.slider(_("Nombre de documents Ã  rÃ©cupÃ©rer"), 1, 10, 2)
st.sidebar.markdown(_(f"**ModÃ¨le**: {MISTRAL_MODEL_NAME}"))

st.sidebar.markdown("---")
st.sidebar.header(_("ğŸ“Š Ã€ propos du dataset"))
st.sidebar.markdown(_("""
**SQuAD 2.0** (Stanford Question Answering Dataset) contient :
- Plus de 100 000 questions
- Questions sur des articles Wikipedia
- Domaines variÃ©s : histoire, science, gÃ©ographie, culture, etc.

**Exemples de sujets :**
- ğŸ›ï¸ Histoire et politique
- ğŸ”¬ Sciences et technologie
- ğŸŒ GÃ©ographie
- ğŸ­ Arts et littÃ©rature
- âš½ Sports
"""))

@st.cache_data
def load_example_questions():
    """Charge quelques exemples de questions depuis le vectorstore"""
    try:
        df = pd.read_csv("squad_2.0/train.csv")
        examples = df.sample(min(10, len(df)))["question"].tolist()
        return examples
    except:
        return [
            "What is the capital of France?",
            "Who invented the telephone?",
            "When did World War II end?",
            "What is photosynthesis?",
            "Who wrote Romeo and Juliet?",
            'What Robert Redford movie was shot here in 1002?',
            'What has the process of revolution in the UK did?', 
            'How long have railroads been important since in Montana'
        ]

example_questions = load_example_questions()

st.header(_("â“ Posez votre question"))
with st.expander(_("ğŸ’¡ Exemples de questions du dataset SQuAD"), expanded=False):
    st.markdown(_("Voici quelques exemples de questions auxquelles le systÃ¨me peut rÃ©pondre :"))
    cols = st.columns(2)

    for i, example in enumerate(example_questions[:8]):
        col_idx = i % 2
        with cols[col_idx]:
            if st.button(f"ğŸ“Œ {_(example[:60], 'en')}{'...' if len(example) > 60 else ''}", key=f"example_{i}"):
                st.session_state.query_input = example
                st.session_state.selected_question = example
                st.rerun()

default_query = st.session_state.get('selected_question', '')
query = st.text_input(
    _("Entrez votre question:"), 
    value=default_query,
    placeholder=_("Ex: What is the capital of France?"),
)

col1, col2 = st.columns([1, 5])
with col1:
    submit_button = st.button(_("ğŸ” Rechercher"), type="primary")
with col2:
    clear_button = st.button(_("ğŸ—‘ï¸ Effacer l'historique"))

if clear_button:
    st.session_state.history = []
    if 'selected_question' in st.session_state:
        del st.session_state.selected_question
    st.rerun()

if submit_button and query and query.strip():
    with st.spinner(_("ğŸ”„ GÃ©nÃ©ration de la rÃ©ponse...")):
        try:
            result = rag_query(query, k=num_results)

            motif_principal = r"array\(\['(.*?)'\],"
            motif_secondaire = r"'text':\s*'([^']+)'"

            resultat = re.search(motif_principal, result["response"])

            if not resultat:
                resultat = re.search(motif_secondaire, result["response"])

            if resultat:
                texte_reponse = resultat.group(1)
            else:
                texte_reponse = result["response"]

            st.session_state.history.append({
                "query": query,
                "response": texte_reponse,
                "retrieved_documents": result["retrieved_documents"]
            })
            
            if 'selected_question' in st.session_state:
                del st.session_state.selected_question
            
            st.success(_("âœ… RÃ©ponse gÃ©nÃ©rÃ©e avec succÃ¨s !"))
            st.subheader(_("ğŸ’¬ RÃ©ponse"))
            st.markdown(f"**{_(texte_reponse, 'en')}**")

        except ValueError as e:
            st.error(_(f"âŒ Erreur de configuration: {e}"))
            st.info(_("ğŸ’¡ VÃ©rifiez que votre clÃ© API Mistral est correctement configurÃ©e dans le fichier .env"))
        except Exception as e:
            st.error(_(f"âŒ Une erreur s'est produite: {str(e)}"))
            with st.expander(_("ğŸ” DÃ©tails de l'erreur")):
                st.exception(e)

elif submit_button and (not query or not query.strip()):
    st.warning(_("âš ï¸ Veuillez entrer une question."))

if st.session_state.history:
    st.header(_("ğŸ“š Historique des requÃªtes"))
    for i, item in enumerate(reversed(st.session_state.history)):
        with st.expander(_(f"Query {len(st.session_state.history) - i}: {item['query']}"), expanded=False):
            st.write(_("**RÃ©ponse:**"))
            st.info(_(item["response"], 'en'))

            st.write(_("**Documents rÃ©cupÃ©rÃ©s:**"))
            for j, doc in enumerate(item["retrieved_documents"]):
                motif_doc = r"Title:\s*(.*?)\nContext:\s*(.*?)\nAnswer:\s*(.*?)$"
                resultat = re.search(motif_doc, doc.page_content, re.DOTALL)
                
                question = doc.metadata.get("question", "Question non disponible")
                
                if resultat:
                    titre = resultat.group(1)
                    contexte = resultat.group(2)
                    reponse_brute = resultat.group(3)
                    
                    motif_reponse = r"'text'\s*:\s*array\(\['(.*?)'\]"
                    match_reponse = re.search(motif_reponse, reponse_brute)
                    reponse = match_reponse.group(1) if match_reponse else reponse_brute
                    
                    with st.expander(f"ğŸ“„ Document {j+1} - {_(titre, 'en')}", expanded=False):
                        st.markdown(f"**{_('Question')}:** {_(question, 'en')}")
                        st.markdown(f"**{_('Contexte')}:**")
                        st.write(_(contexte, 'en'))
                        st.markdown(f"**{_('RÃ©ponse')}:** {_(reponse, 'en')}")
                        st.divider()
                else:
                    with st.expander(f"ğŸ“„ Document {j+1}", expanded=False):
                        st.markdown(f"**{_('Question')}:** {_(question, 'en')}")
                        st.write(doc.page_content)
                        st.divider()

st.markdown("---")

# Section Architecture et Projet
st.title(_("Retrieval Augmented Generation avec Azure OpenAI et Stockage Blob"))

with st.container():
    st.header(_("1. PrÃ©sentation du Projet"))
    st.markdown(_("""
    Bien que la partie du haut ne soit pas fait avec azure, le projet initile est basÃ© sur azure.
    Le cÃ´tÃ© interactif au dessus sert a crÃ©er une interface web pour tester le projet, afin de mieux comprendre le fonctionnement.
    Ce projet vise Ã  concevoir un pipeline RAG (Retrieval Augmented Generation) sur la plateforme Azure. 
    L'objectif est de permettre Ã  un chatbot basÃ© sur Azure OpenAI de rÃ©pondre de maniÃ¨re pertinente Ã  des 
    questions Ã  partir de donnÃ©es externes stockÃ©es dans Azure Blob Storage et indexÃ©es dans Azure Cognitive Search.
    Les outils gratuit utilisÃ©s ont un fonctionnement similaire Ã  ceux d'Azure, permettant ainsi de simuler le pipeline RAG.
                  
    L'interet d'un rag est qu'il permet d'avoir une rÃ©ponse plus pertinente car le LLM va se baser sur des donnÃ©es externes. Ainsi on minimise les hallucinations du modÃ¨le de langage.
    Dans certains de mes projets j'ai eu l'occasion d'ajouter du contexte externe Ã  un modÃ¨le de langage, de facon automatique, et le rÃ©sultat est toujours plus pertinent.
                  
    Le RAG va trouver des documents pertinents dans une base de donnÃ©es externe, avant que le modÃ¨le de langage recoive la question. Le modÃ¨le de langage va ensuite gÃ©nÃ©rer une rÃ©ponse en se basant sur les documents rÃ©cupÃ©rÃ©s.
    """))

with st.container():
    st.header(_("2. Architecture GÃ©nÃ©rale"))
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown(_("""
        **Composants principaux :**
        
        - **Azure Blob Storage** : hÃ©berge les fichiers CSV utilisÃ©s comme source de connaissance 
          (ex. : base de donnÃ©es des vins)
        
        - **Azure Cognitive Search (aka Azure IA Search)** : gÃ¨re l'indexation et la recherche vectorielle grÃ¢ce aux embeddings
        
        - **Azure OpenAI Service** : fournit les modÃ¨les de gÃ©nÃ©ration et d'embedding via les dÃ©ploiements Chatgpt.
        
        - **LangChain** : gÃ¨re l'orchestration entre recherche, vectorisation et gÃ©nÃ©ration de texte, c'est a dire l'orchestration du pipeline RAG.
        
        """))
    
    with col2:
        st.image("templates/assets/architecture-diagram.png", caption=_("Architecture du pipeline RAG"), use_container_width=True)

with st.container():
    st.header(_("3. Ã‰tapes de DÃ©veloppement"))
    
    tabs = st.tabs([
        _("ğŸ“¥ Chargement"),
        _("âœ‚ï¸ PrÃ©paration"),
        _("ğŸ”¢ Embedding"),
        _("ğŸ” Recherche")
    ])
    
    with tabs[0]:
        st.subheader(_("a. Chargement des DonnÃ©es"))
        st.markdown(_("""
        Les fichiers de donnÃ©es (ex : `wine-ratings.csv`) sont importÃ©s depuis le Blob Storage 
        et chargÃ©s avec la classe `CSVLoader` de LangChain.
        """))
        st.code("""
from langchain.document_loaders import CSVLoader

loader = CSVLoader("wine-ratings.csv")
documents = loader.load()
        """, language="python")
    
    with tabs[1]:
        st.subheader(_("b. PrÃ©paration et DÃ©coupage"))
        st.markdown(_("""
        Les documents sont dÃ©coupÃ©s en fragments de 1000 caractÃ¨res pour une indexation efficace 
        via le module `CharacterTextSplitter`.
        """))
        st.code("""
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
        """, language="python")
    
    with tabs[2]:
        st.subheader(_("c. Embedding et Indexation"))
        st.markdown(_("""
        Chaque fragment est converti en vecteur d'embedding Ã  l'aide du modÃ¨le `demo-embedding`. 
        Les vecteurs sont ensuite stockÃ©s dans Azure Cognitive Search.
        """))
        st.code("""
from langchain.embeddings import AzureOpenAIEmbeddings
from langchain.vectorstores import AzureCognitiveSearch

embeddings = AzureOpenAIEmbeddings(
    deployment="demo-embedding"
)
vectorstore = AzureCognitiveSearch.from_documents(
    chunks, embeddings
)
        """, language="python")
    
    with tabs[3]:
        st.subheader(_("d. Recherche et GÃ©nÃ©ration"))
        st.markdown(_("""
        Lorsqu'une requÃªte utilisateur est reÃ§ue, le systÃ¨me exÃ©cute une recherche par similaritÃ© 
        dans l'index et envoie les rÃ©sultats au modÃ¨le GPT Azure (`demo`) pour gÃ©nÃ©rer 
        une rÃ©ponse contextuelle.
        """))
        st.code("""
results = vectorstore.similarity_search_with_relevance_scores(
    query, k=5
)
response = openai.ChatCompletion.create(
    deployment="demo",
    messages=[
        {"role": "system", "content": "Assistant RAG"},
        {"role": "user", "content": f"Context: {results}\\n\\nQuestion: {query}"}
    ]
)
        """, language="python")

with st.container():
    st.header(_("4. FonctionnalitÃ©s du Code"))
    
    features = [
        ("ğŸ”", _("Chargement automatique des clÃ©s API et endpoints Ã  partir d'un fichier `.env`")),
        ("ğŸ”‘", _("Connexion sÃ©curisÃ©e Ã  Azure Search via les variables d'environnement")),
        ("ğŸ“Š", _("Indexation automatique des documents Ã  partir de CSV")),
        ("ğŸ¯", _("Recherche vectorielle avec score de pertinence")),
        ("ğŸ’¬", _("GÃ©nÃ©ration de rÃ©ponses par `openai.ChatCompletion.create()`"))
    ]
    
    for icon, feature in features:
        st.markdown(f"{icon} {feature}")

with st.container():
    st.header(_("5. RÃ©sultats et Performances"))
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label=_("Temps de rÃ©ponse moyen"),
            value="< 2s",
            delta=_("Excellent")
        )
    
    with col2:
        st.metric(
            label=_("Pertinence"),
            value=_("Ã‰levÃ©e"),
            delta=_("CohÃ©rent")
        )
    
    with col3:
        st.metric(
            label=_("Type de donnÃ©es"),
            value=_("Non structurÃ©es"),
            delta=_("Complexes")
        )
    
    st.markdown(_("""
    Le systÃ¨me permet de rÃ©cupÃ©rer et de synthÃ©tiser des informations complexes Ã  partir de bases 
    de donnÃ©es non structurÃ©es. Les rÃ©sultats de similaritÃ© sont pertinents et cohÃ©rents avec les 
    donnÃ©es sources.
    """))

# Section CoÃ»ts
st.header(_("ğŸ“‹ HypothÃ¨ses de Calcul"))

col1, col2, col3 = st.columns(3)
with col1:
    st.metric(_("Taille du Dataset"), "5 Go", _("DonnÃ©es brutes"))
with col2:
    st.metric(_("RequÃªtes/Mois"), "100 000", _("ScÃ©nario modÃ©rÃ© Ã  Ã©levÃ©"))
with col3:
    st.metric(_("Tokens/RequÃªte"), "1 000", "500 entrÃ©e + 500 sortie")

st.markdown(_("""
Les hypothÃ¨ses suivantes servent de base Ã  cette estimation :

- **ModÃ¨le d'Embedding :** `all-MiniLM-L6-v2` 
- **Ratio de Stockage :** Le volume de l'index vectoriel est estimÃ© Ã  **1,5 fois** la taille des donnÃ©es brutes (soit 7,5 Go)
- **FrÃ©quence des RequÃªtes LLM :** 100 000 requÃªtes par mois
- **Taille Moyenne RequÃªte/RÃ©ponse :** 1 000 jetons par requÃªte (500 jetons d'entrÃ©e + 500 jetons de sortie)
"""))

st.header(_("ğŸ—ƒï¸ CoÃ»ts Fixes (Infrastructure Azure)"))

infrastructure_data = {
    _("Composant Azure"): [
        "Azure AI Search",
        "Azure Blob Storage",
        "Azure App Service",
        "**TOTAL**"
    ],
    _("RÃ´le"): [
        _("Base de donnÃ©es vectorielle et recherche"),
        _("Stockage des donnÃ©es brutes (5 Go)"),
        _("HÃ©bergement de l'application Streamlit"),
        ""
    ],
    _("Niveau de Service"): [
        "Basic Tier (1 Search Unit)",
        "Standard LRS - Hot Tier",
        "Basic Tier (B1)",
        ""
    ],
    _("CoÃ»t Mensuel (USD)"): [
        73.73,
        0.09,
        54.75,
        128.57
    ]
}

df_infrastructure = pd.DataFrame(infrastructure_data)

st.dataframe(
    df_infrastructure,
    use_container_width=True,
    hide_index=True,
    column_config={
        _("CoÃ»t Mensuel (USD)"): st.column_config.NumberColumn(format="$%.2f")
    }
)

st.markdown(_("""
**Justification :**
- **Azure AI Search (Basic Tier)** : 73,73 $/mois - Offre 5 Go de quota vectoriel et 15 Go de stockage total, suffisant pour 5 Go de donnÃ©es brutes
- **Azure Blob Storage** : 0,09 $/mois - CoÃ»t trÃ¨s faible pour 5 Go (0,018 $/Go/mois en Hot Tier)
- **Azure App Service (B1)** : 54,75 $/mois - 1 CPU, 1.75 GB RAM pour une application de production lÃ©gÃ¨re avec SLA
"""))

st.header(_("ğŸ¤– CoÃ»ts Variables (Utilisation du LLM)"))

st.markdown(_("""
**ScÃ©nario : 100 000 RequÃªtes/Mois**
- **Total Tokens d'EntrÃ©e :** 100,000 Ã— 500 = 50,000,000 jetons (50 Millions)
- **Total Tokens de Sortie :** 100,000 Ã— 500 = 50,000,000 jetons (50 Millions)
"""))

llm_data = {
    _("ModÃ¨le LLM"): [
        "Mistral Large (Azure)",
        "GPT-4o (OpenAI/Azure)",
        "Mistral Tiny (API Directe)"
    ],
    _("Prix Input ($/1M)"): [4.00, 2.50, 0.14],
    _("Prix Output ($/1M)"): [12.00, 10.00, 0.14],
    _("CoÃ»t Input (50M)"): [200.00, 125.00, 7.50],
    _("CoÃ»t Output (50M)"): [600.00, 500.00, 7.50],
    _("CoÃ»t Total (USD)"): [800.00, 625.00, 15.00]
}

df_llm = pd.DataFrame(llm_data)

st.dataframe(
    df_llm,
    use_container_width=True,
    hide_index=True,
    column_config={
        _("Prix Input ($/1M)"): st.column_config.NumberColumn(format="$%.2f"),
        _("Prix Output ($/1M)"): st.column_config.NumberColumn(format="$%.2f"),
        _("CoÃ»t Input (50M)"): st.column_config.NumberColumn(format="$%.2f"),
        _("CoÃ»t Output (50M)"): st.column_config.NumberColumn(format="$%.2f"),
        _("CoÃ»t Total (USD)"): st.column_config.NumberColumn(format="$%.2f")
    }
)

st.markdown(_("""
**Note :** Le coÃ»t de **Mistral Tiny** est inclus Ã  titre de rÃ©fÃ©rence, car c'est le modÃ¨le que vous utilisez actuellement. 
Il est environ **40 fois moins cher** que GPT-4o, mais sa performance est significativement infÃ©rieure aux modÃ¨les de pointe.
"""))

st.header(_("ğŸ“Š SynthÃ¨se des CoÃ»ts Mensuels Totaux"))

summary_data = {
    _("ScÃ©nario"): [
        "Azure RAG + Mistral Large",
        "Azure RAG + GPT-4o",
        "RAG Local + Mistral Tiny"
    ],
    _("CoÃ»t Fixe (Infrastructure)"): [128.57, 128.57, 0.00],
    _("CoÃ»t Variable (LLM)"): [800.00, 625.00, 15.00],
    _("CoÃ»t Total Mensuel (USD)"): [928.57, 753.57, 15.00]
}

df_summary = pd.DataFrame(summary_data)

st.dataframe(
    df_summary,
    use_container_width=True,
    hide_index=True,
    column_config={
        _("CoÃ»t Fixe (Infrastructure)"): st.column_config.NumberColumn(format="$%.2f"),
        _("CoÃ»t Variable (LLM)"): st.column_config.NumberColumn(format="$%.2f"),
        _("CoÃ»t Total Mensuel (USD)"): st.column_config.NumberColumn(format="$%.2f")
    }
)

col1, col2 = st.columns(2)

with col1:
    st.subheader(_("ğŸ’µ RÃ©partition des CoÃ»ts (Mistral Large)"))
    
    costs_mistral = [128.57, 800.00]
    labels_mistral = [_("Infrastructure"), _("LLM (Mistral Large)")]
    colors = ["#3498db", "#e74c3c"]
    
    fig_pie = go.Figure(data=[go.Pie(
        labels=labels_mistral,
        values=costs_mistral,
        marker=dict(colors=colors),
        textposition="inside",
        textinfo="label+percent+value"
    )])
    
    fig_pie.update_layout(
        title=_("CoÃ»t Total : 928,57 $/mois"),
        height=400,
        showlegend=True
    )
    
    st.plotly_chart(fig_pie, use_container_width=True)

with col2:
    st.subheader(_("ğŸ“ˆ Comparaison des ScÃ©narios"))
    
    scenarios = ["Mistral Large\n(Azure)", "GPT-4o\n(Azure)", "Mistral Tiny\n(Local)"]
    costs = [928.57, 753.57, 15.00]
    colors_bar = ["#e74c3c", "#f39c12", "#27ae60"]
    
    fig_bar = go.Figure(data=[go.Bar(
        x=scenarios,
        y=costs,
        marker=dict(color=colors_bar),
        text=[f"${c:.2f}" for c in costs],
        textposition="outside"
    )])
    
    fig_bar.update_layout(
        title=_("CoÃ»t Total Mensuel par ScÃ©nario"),
        yaxis_title=_("CoÃ»t (USD)"),
        height=400,
        showlegend=False
    )
    
    st.plotly_chart(fig_bar, use_container_width=True)

st.header(_("ğŸ¯ Analyse et Conclusion"))

st.markdown(_("""
### Points ClÃ©s

**1. CoÃ»t d'Infrastructure Fixe :** Environ **130 $ par mois**
   - Le niveau **Basic** d'Azure AI Search est suffisant pour gÃ©rer les 5 Go de donnÃ©es
   - Les coÃ»ts de stockage et d'hÃ©bergement sont nÃ©gligeables en comparaison

**2. Facteur de CoÃ»t Dominant : Le LLM**
   - Le LLM reprÃ©sente plus de **80 %** du coÃ»t total dans un scÃ©nario d'utilisation modÃ©rÃ©e
   - Le choix du modÃ¨le a un impact considÃ©rable sur le budget mensuel

**3. Comparaison des ModÃ¨les**
   - **Mistral Large (Azure)** : 928,57 $/mois - ModÃ¨le de pointe avec bonne performance
   - **GPT-4o (Azure)** : 753,57 $/mois - LÃ©gÃ¨rement moins cher malgrÃ© un prix input plus faible
   - **Mistral Tiny (Local)** : 15,00 $/mois - ExtrÃªmement Ã©conomique, mais performance limitÃ©e

**4. Observation IntÃ©ressante**
   - Sur Azure, **GPT-4o** est plus Ã©conomique que **Mistral Large**, bien que l'API directe de Mistral Large soit gÃ©nÃ©ralement moins chÃ¨re
   - Cela souligne l'importance des **frais de plateforme Azure** pour les modÃ¨les tiers

### Limitations de cette Estimation

- Les coÃ»ts sont basÃ©s sur un scÃ©nario de **100 000 requÃªtes/mois**. Votre utilisation rÃ©elle peut varier
- Les prix Azure peuvent fluctuer ; consultez le [Calculateur de Prix Azure](https://azure.microsoft.com/en-us/pricing/calculator/) pour les estimations les plus rÃ©centes
- Les coÃ»ts de bande passante sortante (egress) ne sont pas inclus dans cette estimation
- Les coÃ»ts de dÃ©veloppement, de maintenance et de support ne sont pas pris en compte
"""))

st.markdown("---")
st.markdown(_(
    """
    DÃ©veloppÃ© par [Gabriel Marie-Brisson](https://gabriel.mariebrisson.fr)
    """
))