import os
import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from mistralai import Mistral
from dotenv import load_dotenv
from numpy import ndarray
import numpy as np
import re

from deep_translator import GoogleTranslator

LANGUAGES = {
    "fr": "ğŸ‡«ğŸ‡· FranÃ§ais",
    "en": "ğŸ‡¬ğŸ‡§ English",
    "es": "ğŸ‡ªğŸ‡¸ EspaÃ±ol",
    "de": "ğŸ‡©ğŸ‡ª Deutsch",
    "it": "ğŸ‡®ğŸ‡¹ Italiano",
    "pt": "ğŸ‡µğŸ‡¹ PortuguÃªs",
    "ja": "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª",
    "zh-CN": "ğŸ‡¨ğŸ‡³ ä¸­æ–‡",
    "ar": "ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "ru": "ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹"
}

# Initialisation de la langue
if 'language' not in st.session_state:
    st.session_state.language = 'fr'

# SÃ©lecteur de langue
lang = st.sidebar.selectbox(
    "ğŸŒ Language / Langue", 
    options=list(LANGUAGES.keys()),
    format_func=lambda x: LANGUAGES[x],
    index=list(LANGUAGES.keys()).index(st.session_state.language)
)

st.session_state.language = lang

# Cache pour les traductions (Ã©vite de retranduire Ã  chaque fois)
if 'translations_cache' not in st.session_state:
    st.session_state.translations_cache = {}

def _(text, source_lang='fr'):
    """Fonction de traduction automatique avec cache"""
    
    # Si la langue cible est la mÃªme que la source, pas de traduction
    if lang == source_lang:
        return text
    
    # VÃ©rifier le cache
    cache_key = f"{source_lang}_{lang}_{text}"
    if cache_key in st.session_state.translations_cache:
        return st.session_state.translations_cache[cache_key]
    
    # Traduire
    try:
        translated = GoogleTranslator(source=source_lang, target=lang).translate(text)
        st.session_state.translations_cache[cache_key] = translated
        return translated
    except:
        return text

load_dotenv()
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL_NAME = os.getenv("MISTRAL_MODEL_NAME", "mistral-tiny-2407")


@st.cache_resource
def load_vectorstore():
    """Charge le vectorstore FAISS sauvegardÃ©"""
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    return vectorstore

try:
    vectorstore = load_vectorstore()
except Exception as e:
    st.error(_(f"âŒ Erreur lors du chargement du vectorstore: {e}"))
    st.stop()

def rag_query(query, k=5):
    """Fonction RAG utilisant Mistral API"""
    if not MISTRAL_API_KEY or MISTRAL_API_KEY.strip() == "":
        raise ValueError(_("MISTRAL_API_KEY is not set or is empty. Please set it in your .env file."))
    
    results = vectorstore.similarity_search(query, k=k)
    
    context = "\n\n".join([doc.page_content for doc in results])
    
    messages = [
        {"role": "system", "content": "You are a helpful assistant. Write responses in complete, well-developed sentences. Express ideas clearly and naturally, avoiding overly brief or list-style answers."},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]
    
    with Mistral(api_key=MISTRAL_API_KEY) as mistral:
        response = mistral.chat.complete(
            model=MISTRAL_MODEL_NAME, 
            messages=messages, 
            stream=False
        )
    
    return {
        "response": response.choices[0].message.content,
        "retrieved_documents": results,
        "context": context
    }

# Bouton de redirection
st.markdown(
    f"""
    <a href="https://gabriel.mariebrisson.fr" target="_blank" style="text-decoration:none;">
    <div style="
    display: inline-block;
    background: linear-gradient(135deg, #6A11CB 0%, #2575FC 100%);
    color: white;
    padding: 12px 25px;
    border-radius: 30px;
    text-align: center;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    box-shadow: 0 4px 15px rgba(37, 117, 252, 0.3);
    transition: all 0.3s ease;
    text-transform: uppercase;
    letter-spacing: 1px;
    border: 2px solid transparent;
    position: relative;
    overflow: hidden;
    ">
    {_("Retour")}
    </div>
    </a>
    """,
    unsafe_allow_html=True
)

st.set_page_config(page_title=_("RAG System with SQuAD"), layout="wide")

st.title(_("RAG System with SQuAD Dataset"))
st.markdown(_("""
Il s'agit d'un systÃ¨me de gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG) construit avec :
- **ModÃ¨le d'embedding** : sentence-transformers/all-MiniLM-L6-v2  
- **Base de donnÃ©es vectorielle** : FAISS (LangChain)  
- **ModÃ¨le de langage (LLM)** : API Mistral  
- **Jeu de donnÃ©es** : SQuAD 2.0
"""))


if not MISTRAL_API_KEY:
    st.error(_("âš ï¸ MISTRAL_API_KEY n'est pas dÃ©finie. Veuillez la configurer dans votre fichier .env"))
    st.stop()

if 'history' not in st.session_state:
    st.session_state.history = []

st.sidebar.header(_("âš™ï¸ ParamÃ¨tres"))
num_results = st.sidebar.slider(_("Nombre de documents Ã  rÃ©cupÃ©rer"), 1, 10, 2)
st.sidebar.markdown(_(f"**ModÃ¨le**: {MISTRAL_MODEL_NAME}"))

st.sidebar.markdown("---")
st.sidebar.header(_("ğŸ“Š Ã€ propos du dataset"))
st.sidebar.markdown(_("""
**SQuAD 2.0** (Stanford Question Answering Dataset) contient :
- Plus de 100 000 questions
- Questions sur des articles Wikipedia
- Domaines variÃ©s : histoire, science, gÃ©ographie, culture, etc.

**Exemples de sujets :**
- ğŸ›ï¸ Histoire et politique
- ğŸ”¬ Sciences et technologie
- ğŸŒ GÃ©ographie
- ğŸ­ Arts et littÃ©rature
- âš½ Sports
"""))

@st.cache_data
def load_example_questions():
    """Charge quelques exemples de questions depuis le vectorstore"""
    import pandas as pd
    try:
        df = pd.read_csv("squad_2.0/train.csv")

        examples = df.sample(min(10, len(df)))["question"].tolist()
        return examples
    except:
        return [
            "What is the capital of France?",
            "Who invented the telephone?",
            "When did World War II end?",
            "What is photosynthesis?",
            "Who wrote Romeo and Juliet?",
            'What Robert Redford movie was shot here in 1002?',
            'What has the process of revolution in the UK did?', 
            'How long have railroads been important since in Montana'
        ]


example_questions = load_example_questions()

st.header(_("â“ Posez votre question"))
with st.expander(_("ğŸ’¡ Exemples de questions du dataset SQuAD"), expanded=False):
    st.markdown(_("Voici quelques exemples de questions auxquelles le systÃ¨me peut rÃ©pondre :"))
    cols = st.columns(2)

    for i, example in enumerate(example_questions[:8]):
        col_idx = i % 2
        with cols[col_idx]:
            if st.button(f"ğŸ“Œ {_(example[:60], 'en')}{'...' if len(example) > 60 else ''}", key=f"example_{i}"):
                st.session_state.query_input = example
                st.session_state.selected_question = example
                st.rerun()

    
default_query = st.session_state.get('selected_question', '')
query = st.text_input(
    _("Entrez votre question:"), 
    value=default_query,
    placeholder=_("Ex: What is the capital of France?"),
)

col1, col2 = st.columns([1, 5])
with col1:
    submit_button = st.button(_("ğŸ” Rechercher"), type="primary")
with col2:
    clear_button = st.button(_("ğŸ—‘ï¸ Effacer l'historique"))

if clear_button:
    st.session_state.history = []
    if 'selected_question' in st.session_state:
        del st.session_state.selected_question
    st.rerun()

if submit_button and query and query.strip():
    with st.spinner(_("ğŸ”„ GÃ©nÃ©ration de la rÃ©ponse...")):
        try:
            result = rag_query(query, k=num_results)

            motif_principal = r"array\(\['(.*?)'\],"
            motif_secondaire = r"'text':\s*'([^']+)'"

            resultat = re.search(motif_principal, result["response"])

            if not resultat:
                resultat = re.search(motif_secondaire, result["response"])

            # Si on nâ€™a trouvÃ© aucun match, on prend directement la rÃ©ponse brute
            if resultat:
                texte_reponse = resultat.group(1)
            else:
                texte_reponse = result["response"]

            # Sauvegarde dans lâ€™historique
            st.session_state.history.append({
                "query": query,
                "response": texte_reponse,
                "retrieved_documents": result["retrieved_documents"]
            })
            
            if 'selected_question' in st.session_state:
                del st.session_state.selected_question
            
            st.success(_("âœ… RÃ©ponse gÃ©nÃ©rÃ©e avec succÃ¨s !"))
            st.subheader(_("ğŸ’¬ RÃ©ponse"))

            st.markdown(f"**{_(texte_reponse, 'en')}**")


        
        except ValueError as e:
            st.error(_(f"âŒ Erreur de configuration: {e}"))
            st.info(_("ğŸ’¡ VÃ©rifiez que votre clÃ© API Mistral est correctement configurÃ©e dans le fichier .env"))
        except Exception as e:
            st.error(_(f"âŒ Une erreur s'est produite: {str(e)}"))
            with st.expander(_("ğŸ” DÃ©tails de l'erreur")):
                st.exception(e)

elif submit_button and (not query or not query.strip()):
    st.warning(_("âš ï¸ Veuillez entrer une question."))

if st.session_state.history:
    st.header(_("ğŸ“š Historique des requÃªtes"))
    for i, item in enumerate(reversed(st.session_state.history)):
        with st.expander(_(f"Query {len(st.session_state.history) - i}: {item['query']}"), expanded=False):
            st.write(_("**RÃ©ponse:**"))
            st.info(_(item["response"], 'en'))

            
            st.write(_("**Documents rÃ©cupÃ©rÃ©s:**"))
            for j, doc in enumerate(item["retrieved_documents"]):
                # Extraire les informations du document
                motif_doc = r"Title:\s*(.*?)\nContext:\s*(.*?)\nAnswer:\s*(.*?)$"
                resultat = re.search(motif_doc, doc.page_content, re.DOTALL)
                
                # RÃ©cupÃ©rer la question depuis les mÃ©tadonnÃ©es
                question = doc.metadata.get("question", "Question non disponible")
                
                if resultat:
                    titre = resultat.group(1)
                    contexte = resultat.group(2)
                    reponse_brute = resultat.group(3)
                    
                    # Extraire le texte de la rÃ©ponse depuis le dictionnaire
                    motif_reponse = r"'text'\s*:\s*array\(\['(.*?)'\]"
                    match_reponse = re.search(motif_reponse, reponse_brute)
                    reponse = match_reponse.group(1)
                    
                    with st.expander(f"ğŸ“„ Document {j+1} - {_(titre, 'en')}", expanded=False):
                        st.markdown(f"**{_('Question')}:** {_(question, 'en')}")
                        st.markdown(f"**{_('Contexte')}:**")
                        st.write(_(contexte, 'en'))
                        st.markdown(f"**{_('RÃ©ponse')}:** {_(reponse, 'en')}")
                        st.divider()
                else:
                    # Fallback si le regex ne match pas
                    with st.expander(f"ğŸ“„ Document {j+1}", expanded=False):
                        st.markdown(f"**{_('Question')}:** {_(question, 'en')}")
                        st.write(doc.page_content)
                        st.divider()

st.markdown("---")
st.markdown(_(
    """
    DÃ©veloppÃ© par [Gabriel Marie-Brisson](https://gabriel.mariebrisson.fr)
    """
))