import os
import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from mistralai import Mistral
from dotenv import load_dotenv
from numpy import ndarray
import numpy as np
import re

from deep_translator import GoogleTranslator

LANGUAGES = {
    "fr": "ğŸ‡«ğŸ‡· FranÃ§ais",
    "en": "ğŸ‡¬ğŸ‡§ English",
    "es": "ğŸ‡ªğŸ‡¸ EspaÃ±ol",
    "de": "ğŸ‡©ğŸ‡ª Deutsch",
    "it": "ğŸ‡®ğŸ‡¹ Italiano",
    "pt": "ğŸ‡µğŸ‡¹ PortuguÃªs",
    "ja": "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª",
    "zh-CN": "ğŸ‡¨ğŸ‡³ ä¸­æ–‡",
    "ar": "ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "ru": "ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹"
}

# Initialisation de la langue
if 'language' not in st.session_state:
    st.session_state.language = 'fr'

# SÃ©lecteur de langue
lang = st.sidebar.selectbox(
    "ğŸŒ Language / Langue", 
    options=list(LANGUAGES.keys()),
    format_func=lambda x: LANGUAGES[x],
    index=list(LANGUAGES.keys()).index(st.session_state.language)
)

st.session_state.language = lang

# Cache pour les traductions (Ã©vite de retranduire Ã  chaque fois)
if 'translations_cache' not in st.session_state:
    st.session_state.translations_cache = {}

def _(text, source_lang='fr'):
    """Fonction de traduction automatique avec cache"""
    
    # Si la langue cible est la mÃªme que la source, pas de traduction
    if lang == source_lang:
        return text
    
    # VÃ©rifier le cache
    cache_key = f"{source_lang}_{lang}_{text}"
    if cache_key in st.session_state.translations_cache:
        return st.session_state.translations_cache[cache_key]
    
    # Traduire
    try:
        translated = GoogleTranslator(source=source_lang, target=lang).translate(text)
        st.session_state.translations_cache[cache_key] = translated
        return translated
    except:
        return text

load_dotenv()
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL_NAME = os.getenv("MISTRAL_MODEL_NAME", "mistral-tiny-2407")


@st.cache_resource
def load_vectorstore():
    """Charge le vectorstore FAISS sauvegardÃ©"""
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    return vectorstore

try:
    vectorstore = load_vectorstore()
except Exception as e:
    st.error(_(f"âŒ Erreur lors du chargement du vectorstore: {e}"))
    st.stop()

def rag_query(query, k=5):
    """Fonction RAG utilisant Mistral API"""
    if not MISTRAL_API_KEY or MISTRAL_API_KEY.strip() == "":
        raise ValueError(_("MISTRAL_API_KEY is not set or is empty. Please set it in your .env file."))
    
    results = vectorstore.similarity_search(query, k=k)
    
    context = "\n\n".join([doc.page_content for doc in results])
    
    messages = [
        {"role": "system", "content": "You are a helpful assistant. Write responses in complete, well-developed sentences. Express ideas clearly and naturally, avoiding overly brief or list-style answers."},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]
    
    with Mistral(api_key=MISTRAL_API_KEY) as mistral:
        response = mistral.chat.complete(
            model=MISTRAL_MODEL_NAME, 
            messages=messages, 
            stream=False
        )
    
    return {
        "response": response.choices[0].message.content,
        "retrieved_documents": results,
        "context": context
    }

# Bouton de redirection
st.markdown(
    f"""
    <a href="https://gabriel.mariebrisson.fr" target="_blank" style="text-decoration:none;">
    <div style="
    display: inline-block;
    background: linear-gradient(135deg, #6A11CB 0%, #2575FC 100%);
    color: white;
    padding: 12px 25px;
    border-radius: 30px;
    text-align: center;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    box-shadow: 0 4px 15px rgba(37, 117, 252, 0.3);
    transition: all 0.3s ease;
    text-transform: uppercase;
    letter-spacing: 1px;
    border: 2px solid transparent;
    position: relative;
    overflow: hidden;
    ">
    {_("Retour")}
    </div>
    </a>
    """,
    unsafe_allow_html=True
)

st.set_page_config(page_title=_("RAG System with SQuAD"), layout="wide")

st.title(_("RAG System with SQuAD Dataset"))
st.markdown(_("""
Il s'agit d'un systÃ¨me de gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG) construit avec :
- **ModÃ¨le d'embedding** : sentence-transformers/all-MiniLM-L6-v2  
- **Base de donnÃ©es vectorielle** : FAISS (LangChain)  
- **ModÃ¨le de langage (LLM)** : API Mistral  
- **Jeu de donnÃ©es** : SQuAD 2.0
"""))


if not MISTRAL_API_KEY:
    st.error(_("âš ï¸ MISTRAL_API_KEY n'est pas dÃ©finie. Veuillez la configurer dans votre fichier .env"))
    st.stop()

if 'history' not in st.session_state:
    st.session_state.history = []

st.sidebar.header(_("âš™ï¸ ParamÃ¨tres"))
num_results = st.sidebar.slider(_("Nombre de documents Ã  rÃ©cupÃ©rer"), 1, 10, 2)
st.sidebar.markdown(_(f"**ModÃ¨le**: {MISTRAL_MODEL_NAME}"))

st.sidebar.markdown("---")
st.sidebar.header(_("ğŸ“Š Ã€ propos du dataset"))
st.sidebar.markdown(_("""
**SQuAD 2.0** (Stanford Question Answering Dataset) contient :
- Plus de 100 000 questions
- Questions sur des articles Wikipedia
- Domaines variÃ©s : histoire, science, gÃ©ographie, culture, etc.

**Exemples de sujets :**
- ğŸ›ï¸ Histoire et politique
- ğŸ”¬ Sciences et technologie
- ğŸŒ GÃ©ographie
- ğŸ­ Arts et littÃ©rature
- âš½ Sports
"""))

@st.cache_data
def load_example_questions():
    """Charge quelques exemples de questions depuis le vectorstore"""
    import pandas as pd
    try:
        df = pd.read_csv("squad_2.0/train.csv")

        examples = df.sample(min(10, len(df)))["question"].tolist()
        return examples
    except:
        return [
            "What is the capital of France?",
            "Who invented the telephone?",
            "When did World War II end?",
            "What is photosynthesis?",
            "Who wrote Romeo and Juliet?",
            'What Robert Redford movie was shot here in 1002?',
            'What has the process of revolution in the UK did?', 
            'How long have railroads been important since in Montana'
        ]


example_questions = load_example_questions()

st.header(_("â“ Posez votre question"))
with st.expander(_("ğŸ’¡ Exemples de questions du dataset SQuAD"), expanded=False):
    st.markdown(_("Voici quelques exemples de questions auxquelles le systÃ¨me peut rÃ©pondre :"))
    cols = st.columns(2)

    for i, example in enumerate(example_questions[:8]):
        col_idx = i % 2
        with cols[col_idx]:
            if st.button(f"ğŸ“Œ {_(example[:60], 'en')}{'...' if len(example) > 60 else ''}", key=f"example_{i}"):
                st.session_state.query_input = example
                st.session_state.selected_question = example
                st.rerun()

    
default_query = st.session_state.get('selected_question', '')
query = st.text_input(
    _("Entrez votre question:"), 
    value=default_query,
    placeholder=_("Ex: What is the capital of France?"),
)

col1, col2 = st.columns([1, 5])
with col1:
    submit_button = st.button(_("ğŸ” Rechercher"), type="primary")
with col2:
    clear_button = st.button(_("ğŸ—‘ï¸ Effacer l'historique"))

if clear_button:
    st.session_state.history = []
    if 'selected_question' in st.session_state:
        del st.session_state.selected_question
    st.rerun()

if submit_button and query and query.strip():
    with st.spinner(_("ğŸ”„ GÃ©nÃ©ration de la rÃ©ponse...")):
        try:
            result = rag_query(query, k=num_results)

            motif_principal = r"array\(\['(.*?)'\],"
            motif_secondaire = r"'text':\s*'([^']+)'"

            resultat = re.search(motif_principal, result["response"])

            if not resultat:
                resultat = re.search(motif_secondaire, result["response"])

            # Si on nâ€™a trouvÃ© aucun match, on prend directement la rÃ©ponse brute
            if resultat:
                texte_reponse = resultat.group(1)
            else:
                texte_reponse = result["response"]

            # Sauvegarde dans lâ€™historique
            st.session_state.history.append({
                "query": query,
                "response": texte_reponse,
                "retrieved_documents": result["retrieved_documents"]
            })
            
            if 'selected_question' in st.session_state:
                del st.session_state.selected_question
            
            st.success(_("âœ… RÃ©ponse gÃ©nÃ©rÃ©e avec succÃ¨s !"))
            st.subheader(_("ğŸ’¬ RÃ©ponse"))

            st.markdown(f"**{_(texte_reponse, 'en')}**")


        
        except ValueError as e:
            st.error(_(f"âŒ Erreur de configuration: {e}"))
            st.info(_("ğŸ’¡ VÃ©rifiez que votre clÃ© API Mistral est correctement configurÃ©e dans le fichier .env"))
        except Exception as e:
            st.error(_(f"âŒ Une erreur s'est produite: {str(e)}"))
            with st.expander(_("ğŸ” DÃ©tails de l'erreur")):
                st.exception(e)

elif submit_button and (not query or not query.strip()):
    st.warning(_("âš ï¸ Veuillez entrer une question."))

if st.session_state.history:
    st.header(_("ğŸ“š Historique des requÃªtes"))
    for i, item in enumerate(reversed(st.session_state.history)):
        with st.expander(_(f"Query {len(st.session_state.history) - i}: {item['query']}"), expanded=False):
            st.write(_("**RÃ©ponse:**"))
            st.info(_(item["response"], 'en'))

            
            st.write(_("**Documents rÃ©cupÃ©rÃ©s:**"))
            for j, doc in enumerate(item["retrieved_documents"]):
                # Extraire les informations du document
                motif_doc = r"Title:\s*(.*?)\nContext:\s*(.*?)\nAnswer:\s*(.*?)$"
                resultat = re.search(motif_doc, doc.page_content, re.DOTALL)
                
                # RÃ©cupÃ©rer la question depuis les mÃ©tadonnÃ©es
                question = doc.metadata.get("question", "Question non disponible")
                
                if resultat:
                    titre = resultat.group(1)
                    contexte = resultat.group(2)
                    reponse_brute = resultat.group(3)
                    
                    # Extraire le texte de la rÃ©ponse depuis le dictionnaire
                    motif_reponse = r"'text'\s*:\s*array\(\['(.*?)'\]"
                    match_reponse = re.search(motif_reponse, reponse_brute)
                    reponse = match_reponse.group(1)
                    
                    with st.expander(f"ğŸ“„ Document {j+1} - {_(titre, 'en')}", expanded=False):
                        st.markdown(f"**{_('Question')}:** {_(question, 'en')}")
                        st.markdown(f"**{_('Contexte')}:**")
                        st.write(_(contexte, 'en'))
                        st.markdown(f"**{_('RÃ©ponse')}:** {_(reponse, 'en')}")
                        st.divider()
                else:
                    # Fallback si le regex ne match pas
                    with st.expander(f"ğŸ“„ Document {j+1}", expanded=False):
                        st.markdown(f"**{_('Question')}:** {_(question, 'en')}")
                        st.write(doc.page_content)
                        st.divider()


# Titre principal
st.title(_("Portfolio de Projet : ImplÃ©mentation d'un SystÃ¨me RAG"))
st.subheader(_("Retrieval Augmented Generation avec Azure OpenAI et Stockage Blob"))

# 1. PrÃ©sentation du Projet
with st.container():
    st.header(_("1. PrÃ©sentation du Projet"))
    st.markdown(_("""
    Ce projet vise Ã  concevoir un pipeline RAG (Retrieval Augmented Generation) sur la plateforme Azure. 
    L'objectif est de permettre Ã  un chatbot basÃ© sur Azure OpenAI de rÃ©pondre de maniÃ¨re pertinente Ã  des 
    questions Ã  partir de donnÃ©es externes stockÃ©es dans Azure Blob Storage et indexÃ©es dans Azure Cognitive Search.
    """))

# 2. Architecture GÃ©nÃ©rale
with st.container():
    st.header(_("2. Architecture GÃ©nÃ©rale"))
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown(_("""
        **Composants principaux :**
        
        - **Azure Blob Storage** : hÃ©berge les fichiers CSV utilisÃ©s comme source de connaissance 
          (ex. : base de donnÃ©es des vins)
        
        - **Azure Cognitive Search** : gÃ¨re l'indexation et la recherche vectorielle grÃ¢ce aux embeddings
        
        - **Azure OpenAI Service** : fournit les modÃ¨les de gÃ©nÃ©ration et d'embedding via les dÃ©ploiements 
          "demo-alfredo" (LLM) et "demo-embedding"
        
        - **LangChain** : gÃ¨re l'orchestration entre recherche, vectorisation et gÃ©nÃ©ration de texte
        """))
    
    with col2:
        # Vous pouvez ajouter l'image ici si vous l'avez
        st.info(_("ğŸ’¡ **Architecture du pipeline RAG**\n\nDonnÃ©es â†’ Blob Storage â†’ Cognitive Search â†’ OpenAI â†’ RÃ©ponse"))

# 3. Ã‰tapes de DÃ©veloppement
with st.container():
    st.header(_("3. Ã‰tapes de DÃ©veloppement"))
    
    tabs = st.tabs([
        _("ğŸ“¥ Chargement"),
        _("âœ‚ï¸ PrÃ©paration"),
        _("ğŸ”¢ Embedding"),
        _("ğŸ” Recherche")
    ])
    
    with tabs[0]:
        st.subheader(_("a. Chargement des DonnÃ©es"))
        st.markdown(_("""
        Les fichiers de donnÃ©es (ex : `wine-ratings.csv`) sont importÃ©s depuis le Blob Storage 
        et chargÃ©s avec la classe `CSVLoader` de LangChain.
        """))
        st.code("""
from langchain.document_loaders import CSVLoader

loader = CSVLoader("wine-ratings.csv")
documents = loader.load()
        """, language="python")
    
    with tabs[1]:
        st.subheader(_("b. PrÃ©paration et DÃ©coupage"))
        st.markdown(_("""
        Les documents sont dÃ©coupÃ©s en fragments de 1000 caractÃ¨res pour une indexation efficace 
        via le module `CharacterTextSplitter`.
        """))
        st.code("""
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
        """, language="python")
    
    with tabs[2]:
        st.subheader(_("c. Embedding et Indexation"))
        st.markdown(_("""
        Chaque fragment est converti en vecteur d'embedding Ã  l'aide du modÃ¨le `demo-embedding`. 
        Les vecteurs sont ensuite stockÃ©s dans Azure Cognitive Search.
        """))
        st.code("""
from langchain.embeddings import AzureOpenAIEmbeddings
from langchain.vectorstores import AzureCognitiveSearch

embeddings = AzureOpenAIEmbeddings(
    deployment="demo-embedding"
)
vectorstore = AzureCognitiveSearch.from_documents(
    chunks, embeddings
)
        """, language="python")
    
    with tabs[3]:
        st.subheader(_("d. Recherche et GÃ©nÃ©ration"))
        st.markdown(_("""
        Lorsqu'une requÃªte utilisateur est reÃ§ue, le systÃ¨me exÃ©cute une recherche par similaritÃ© 
        dans l'index et envoie les rÃ©sultats au modÃ¨le GPT Azure (`demo-alfredo`) pour gÃ©nÃ©rer 
        une rÃ©ponse contextuelle.
        """))
        st.code("""
results = vectorstore.similarity_search_with_relevance_scores(
    query, k=5
)
response = openai.ChatCompletion.create(
    deployment="demo-alfredo",
    messages=[
        {"role": "system", "content": "Assistant RAG"},
        {"role": "user", "content": f"Context: {results}\\n\\nQuestion: {query}"}
    ]
)
        """, language="python")

# 4. FonctionnalitÃ©s du Code
with st.container():
    st.header(_("4. FonctionnalitÃ©s du Code"))
    
    features = [
        ("ğŸ”", _("Chargement automatique des clÃ©s API et endpoints Ã  partir d'un fichier `.env`")),
        ("ğŸ”’", _("Connexion sÃ©curisÃ©e Ã  Azure Search via les variables d'environnement")),
        ("ğŸ“Š", _("Indexation automatique des documents Ã  partir de CSV")),
        ("ğŸ¯", _("Recherche vectorielle avec score de pertinence")),
        ("ğŸ’¬", _("GÃ©nÃ©ration de rÃ©ponses par `openai.ChatCompletion.create()`"))
    ]
    
    for icon, feature in features:
        st.markdown(f"{icon} {feature}")

# 5. Exemple de RÃ©sultat
with st.container():
    st.header(_("5. Exemple de RÃ©sultat"))
    
    st.markdown(_("**RequÃªte :**"))
    st.info(_("What is the best Cabernet Sauvignon wine in Napa Valley above 94 points?"))
    
    st.markdown(_("**RÃ©sultat :**"))
    st.success(_("""
    â†’ Le systÃ¨me renvoie les 5 documents les plus pertinents, extrait le contenu du plus pertinent 
    et l'utilise pour enrichir la rÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le GPT.
    """))

# 6. Outils et Technologies
with st.container():
    st.header(_("6. Outils et Technologies"))
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(_("""
        **Langage :**
        - ğŸ Python
        
        **BibliothÃ¨ques :**
        - ğŸ¦œ LangChain
        - ğŸ¤– OpenAI
        - âš™ï¸ dotenv
        """))
    
    with col2:
        st.markdown(_("""
        **Services Azure :**
        - â˜ï¸ Azure OpenAI
        - ğŸ” Azure Cognitive Search
        - ğŸ“¦ Azure Blob Storage
        
        **Format de donnÃ©es :**
        - ğŸ“Š CSV
        """))

# 7. DifficultÃ©s et Optimisations
with st.container():
    st.header(_("7. DifficultÃ©s et Optimisations"))
    
    challenges = [
        _("Gestion des connexions sÃ©curisÃ©es Ã  Azure via des variables d'environnement"),
        _("Encodage des textes et taille des chunks pour maximiser la pertinence des embeddings"),
        _("Optimisation du scoring vectoriel dans Azure Search pour accÃ©lÃ©rer la recherche")
    ]
    
    for challenge in challenges:
        st.markdown(f"- âš¡ {challenge}")

# 8. RÃ©sultats et Performances
with st.container():
    st.header(_("8. RÃ©sultats et Performances"))
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label=_("Temps de rÃ©ponse moyen"),
            value="< 2s",
            delta=_("Excellent")
        )
    
    with col2:
        st.metric(
            label=_("Pertinence"),
            value=_("Ã‰levÃ©e"),
            delta=_("CohÃ©rent")
        )
    
    with col3:
        st.metric(
            label=_("Type de donnÃ©es"),
            value=_("Non structurÃ©es"),
            delta=_("Complexes")
        )
    
    st.markdown(_("""
    Le systÃ¨me permet de rÃ©cupÃ©rer et de synthÃ©tiser des informations complexes Ã  partir de bases 
    de donnÃ©es non structurÃ©es. Les rÃ©sultats de similaritÃ© sont pertinents et cohÃ©rents avec les 
    donnÃ©es sources.
    """))

# 9. Perspectives d'AmÃ©lioration
with st.container():
    st.header(_("9. Perspectives d'AmÃ©lioration"))
    
    improvements = [
        ("ğŸ”„", _("Connexion Ã  un stockage Blob dynamique pour actualiser l'index en temps rÃ©el")),
        ("ğŸŒ", _("Ajout d'une interface web interactive pour tester le RAG directement depuis le navigateur")),
        ("âš¡", _("IntÃ©gration d'un cache Redis pour rÃ©duire le coÃ»t des requÃªtes rÃ©currentes"))
    ]
    
    for icon, improvement in improvements:
        st.markdown(f"{icon} {improvement}")

        
st.markdown("---")
st.markdown(_(
    """
    DÃ©veloppÃ© par [Gabriel Marie-Brisson](https://gabriel.mariebrisson.fr)
    """
))